import argparse
import yaml
import torch
import numpy as np
import pandas as pd
import random
import os
import gc
from collections import defaultdict
from torch.utils.data import DataLoader

from model import DualTowerTitans
from trainer import StreamingTrainer
from utils import (
    prepare_data_pipeline, 
    RecommendationDataset, 
    UniqueUserBatchSampler
)

def set_seed(seed):
    """å›ºå®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯¦é©—å¯é‡ç¾æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def print_metrics(period_id, metrics, prefix="Eval"):
    """ç¾åŒ–æ‰“å°æŒ‡æ¨™ (é‚„åŸè©³ç´°æ ¼å¼)"""
    if not metrics:
        return
    
    # æå– Loss èˆ‡ AUC
    loss_val = metrics.get('loss', 0.0)
    auc_val = metrics.get('auc', 0.0)
    
    print(f"  [{prefix} Period {period_id}] Results:")
    print(f"  - Loss     : {loss_val:.4f}")
    print(f"  - GAUC     : {auc_val:.4f}") # In-Batch User Average AUC = GAUC
    print("  " + "-"*45)
    
    # è™•ç† Top-K æŒ‡æ¨™
    # æ‰¾å‡ºæ‰€æœ‰çš„ k å€¼
    k_set = set()
    for key in metrics.keys():
        if key.startswith('recall@'):
            k_set.add(int(key.split('@')[1]))
    sorted_ks = sorted(list(k_set))
    
    for k in sorted_ks:
        r_k = metrics.get(f'recall@{k}', 0.0)
        n_k = metrics.get(f'ndcg@{k}', 0.0)
        print(f"  - Recall@{k:<2}: {r_k:.4f}   |   NDCG@{k:<2}: {n_k:.4f}")
        
    print("  " + "-"*45)

def main():
    # 1. Argument Parsing
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, default='config.yaml', help='Path to config file')
    args = parser.parse_args()

    # 2. Load Config
    if not os.path.exists(args.config):
        raise FileNotFoundError(f"Config file not found: {args.config}")
    with open(args.config, 'r') as f:
        cfg = yaml.safe_load(f)

    # 3. Setup
    set_seed(cfg['seed'])
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\n{'='*60}")
    print(f"ğŸš€ Starting Streaming Experiment on {device}")
    print(f"{'='*60}\n")

    # 4. Data Pipeline
    #    é€™æœƒè§¸ç™¼ Cache è®€å–æˆ–é‡æ–°å»ºæ§‹
    full_df, global_meta, _ = prepare_data_pipeline(cfg)
    
    # æ³¨å…¥ Meta
    cfg['model']['n_users'] = global_meta['n_users']
    cfg['model']['n_items'] = global_meta['n_items']
    cfg['model']['n_cates'] = global_meta['n_cates']
    
    # 5. Model & Trainer Init
    model = DualTowerTitans(global_meta, cfg).to(device)

    # === [DEBUG Tool] Register NaN Hooks ===
    def check_nan_hook(module, input, output):
        if isinstance(output, torch.Tensor):
            if torch.isnan(output).any():
                print(f"!!! NaN detected in {module.__class__.__name__} !!!")
                # print(f"Output: {output}") # å°å‡ºæ•¸å€¼æœƒå¤ªå¤šï¼Œå…ˆåªå°å±¤å
                raise RuntimeError(f"NaN detected in layer: {module}")
        elif isinstance(output, tuple):
            for i, out in enumerate(output):
                if isinstance(out, torch.Tensor) and torch.isnan(out).any():
                    print(f"!!! NaN detected in {module.__class__.__name__} output[{i}] !!!")
                    raise RuntimeError(f"NaN detected in layer: {module}")

    # è¨»å†Š Hook åˆ°æ‰€æœ‰å­æ¨¡çµ„
    print("--- [Debug] Registering NaN hooks to all layers ---")
    for name, layer in model.named_modules():
        layer.register_forward_hook(check_nan_hook)
    # =======================================


    trainer = StreamingTrainer(model, cfg, device)

    # 6. Experiment Control Parameters
    exp_cfg = cfg.get('experiment', {})
    
    # å¾è³‡æ–™ä¸­å–å¾—å¯¦éš›æœ€å¤§ Period
    max_data_period = int(full_df['period'].max())
    # è¨­å®šå¯¦é©—çµ‚æ­¢é» (å– Config èˆ‡ Data çš„æœ€å°å€¼)
    target_num_periods = exp_cfg.get('num_periods', 9999)
    end_p = min(target_num_periods, max_data_period + 1)
    
    start_train_p = exp_cfg.get('train_start_period', 0)
    start_test_p = exp_cfg.get('test_start_period', 0)
    
    results_log = []

    print(f"--- Plan: Run Period 0 -> {end_p - 1} ---")
    print(f"--- Train Start (Stage 2): P{start_train_p} | Test Start: P{start_test_p} ---\n")

    # 7. Streaming Loop
    for p_id in range(end_p):
        print(f"\n{'='*40}\n>>> Period {p_id} Processing\n{'='*40}")
        
        # -------------------------------------------------
        # A. Data Slicing
        # -------------------------------------------------
        curr_df = full_df[full_df['period'] == p_id]
        
        # ä¸‹ä¸€å€‹ Period (ç”¨æ–¼ Forward Transfer Eval)
        next_p_id = p_id + 1
        next_df = full_df[full_df['period'] == next_p_id] if next_p_id < (max_data_period + 1) else None

        if curr_df.empty:
            print(f"Period {p_id} is empty. Skipping.")
            continue

        # -------------------------------------------------
        # B. Stage 1: Step Mode (Always Run for Memory Update)
        # -------------------------------------------------
        # å³ä½¿ p_id < start_train_pï¼Œæˆ‘å€‘é€šå¸¸ä¹Ÿè¦è·‘ Stage 1 ä¾†æ›´æ–° Memory Bankï¼Œ
        # å¦å‰‡ Memory æœƒæ˜¯ç©ºçš„ã€‚
        print(f"[Data] Step Loader: {len(curr_df)} interactions")
        
        ds_step = RecommendationDataset(
            curr_df, 
            mode='step', 
            max_seq_len=cfg['model']['max_seq_len'] # é€™è£¡ç”¨ä½œ context window padding
        )
        
        unique_sampler = UniqueUserBatchSampler(
            ds_step.user_ids, 
            batch_size=cfg['train']['batch_size_stage1']
        )
        
        loader_step = DataLoader(
            ds_step,
            batch_sampler=unique_sampler, 
            num_workers=0
        )

        # -------------------------------------------------
        # C. Stage 2: Seq Mode (Conditional)
        # -------------------------------------------------
        loader_seq = None
        # æ¢ä»¶ï¼šé P0 (ç„¡æ­·å²) ä¸” é”åˆ°è¨“ç·´èµ·å§‹æœŸ
        if p_id > 0 and p_id >= start_train_p:
            print(f"[Data] Seq Loader: Preparing sequences...")
            ds_seq = RecommendationDataset(
                curr_df, 
                mode='seq', 
                max_seq_len=cfg['model']['max_seq_len'], # L_max
                context_len=cfg['model']['context_len']  # K
            )
            
            if len(ds_seq) > 0:
                loader_seq = DataLoader(
                    ds_seq,
                    batch_size=cfg['train']['batch_size_stage2'],
                    shuffle=True, 
                    num_workers=0
                )
            else:
                print("[Warning] No valid sequences found for Stage 2.")
        else:
             print(f"[Control] Skipping Stage 2 (Before train_start_period {start_train_p})")

        # -------------------------------------------------
        # D. Eval Loader (Conditional)
        # -------------------------------------------------
        loader_eval = None
        if next_df is not None and not next_df.empty and next_p_id >= start_test_p:
            print(f"[Data] Eval Loader: Next Period {next_p_id} ({len(next_df)} samples)")
            ds_eval = RecommendationDataset(
                next_df,
                mode='step',
                max_seq_len=cfg['model']['max_seq_len']
            )
            
            loader_eval = DataLoader(
                ds_eval,
                batch_size=cfg['train']['batch_size_stage1'],
                shuffle=False,
                num_workers=0,
                drop_last=True
            )
        elif next_p_id < start_test_p:
            print(f"[Control] Skipping Evaluation (Next P{next_p_id} < test_start {start_test_p})")

        # -------------------------------------------------
        # E. Execute Period
        # -------------------------------------------------
        # run_period å…§éƒ¨æœƒæ ¹æ“š loader æ˜¯å¦ç‚º None è‡ªå‹•è·³éç›¸æ‡‰éšæ®µ
        period_metrics = trainer.run_period(
            p_id, 
            loader_step, 
            loader_seq, 
            loader_eval
        )
        
        # Log Results
        if period_metrics:
            # è£œä¸Š Period è³‡è¨Š
            period_metrics['period'] = next_p_id 
            results_log.append(period_metrics)
            print_metrics(next_p_id, period_metrics, prefix="Final Eval")
        
        # Cleanup
        gc.collect()
        torch.cuda.empty_cache()

    # -------------------------------------------------
    # F. Final Summary
    # -------------------------------------------------
    print("\n" + "="*60)
    print(" >>> Experiment Summary")
    print("="*60)
    
    if results_log:
        df_res = pd.DataFrame(results_log)
        mean_res = df_res.mean(numeric_only=True)
        
        print(f"Total Evaluated Periods: {len(df_res)}")
        
        # 1. é¡¯ç¤ºå¹³å‡ GAUC
        if 'auc' in mean_res:
            print(f"\n  [Average GAUC] : {mean_res['auc']:.4f}")
            print("  " + "-"*30)
        
        # 2. é¡¯ç¤º Top-K
        # è§£ææ‰€æœ‰çš„ K
        k_set = set()
        for key in mean_res.index:
            if key.startswith('recall@'):
                k_set.add(int(key.split('@')[1]))
        sorted_ks = sorted(list(k_set))
        
        for k in sorted_ks:
            r_val = mean_res.get(f'recall@{k}', 0.0)
            n_val = mean_res.get(f'ndcg@{k}', 0.0)
            print(f"  Recall@{k:<2} : {r_val:.4f}   |   NDCG@{k:<2} : {n_val:.4f}")
            
    else:
        print("No evaluation metrics collected.")
    print("="*60)

if __name__ == "__main__":
    main()